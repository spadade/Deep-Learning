---
title: "Deep Learning RNN - IMDB data sentiment analysis"
author: "Srushti Padade"
date: "March 1, 2020"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    toc: true
    df_print: paged
---

# **Library**

For the Deep learning we are using Keras libarary.
```{r}
library(keras)
```

# **Labeling**

The IMDB data set is loaded in the environment and train and test data is stored separtely. The data is labelled based on the Positive and Negative reviews.

```{r}
imdb_dir <- "D:/Advance_ML/aclImdb"
train_dir <- file.path(imdb_dir, "train")
labels <- c()
texts <- c()
for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(train_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"),
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}
```

# **Altering the requirements

## Cut 0ff to 150 words

The reviews are cut off after 150 words. Originally we had cut off of 200 words.

```{r}
maxlen <- 150
```

## Reducing Training sample size to 100 samples

Training dataset is kept for 100 samples. Intially the sample size was 10000.

```{r}
training_samples <- 100
```

Validation dataset is of 10,000 samples

```{r}
validation_samples <- 10000
```

The top 10,000 words in the dataset are considered

```{r}
max_words <- 10000
```

# **Text to Tensors**

The imdb review data are tokenized and converted into tensors.

```{r}
tokenizer <- text_tokenizer(num_words = max_words) %>% 
  fit_text_tokenizer(texts)

sequences <- texts_to_sequences(tokenizer, texts)
word_index = tokenizer$word_index

cat("Found", length(word_index), "Unique Tokens.\n")
data <- pad_sequences(sequences, maxlen = maxlen)
y_data <- as.array(labels)
labels <- as.array(labels)
cat("Shape of Data Tensor:", dim(data), "\n")
cat('Shape of Label Tensor:', dim(labels), "\n")
```

# **Data Partitioning**

Splitting the data into training and validation samples by random sampling.

```{r}
set.seed(246)
indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1): 
                                (training_samples + validation_samples)]

x_train <- data[training_indices,]
y_train <- labels[training_indices]

x_val <- data[validation_indices,]
y_val <- labels[validation_indices]
```

# **Pretrainned Network (Word Embedding)**

Loading the Glove Algorithm by loading Pre-processed the embeddings of Glove.
The glove text file consist of the embedding layer where there is 100-dimensional embedding vectors for 400,000 words (or non-word tokens).

```{r}
glove_dir = 'D:/Advance_ML/glove.6B'
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))
```

## **Pre processing the Embedding layer**

```{r}
embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}

cat("Found", length(embeddings_index), "word vectors.\n")
```

## **Embedding Matrix**

Building an embedding matrix that can be loaded into an embedding layer

```{r}
embedding_dim <- 100
embedding_matrix <- array(0, c(max_words, embedding_dim))

for (word in names(word_index)) {
  index <- word_index[[word]]
  if (index < max_words) {
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
      # Words not found in the embedding index will be all zeros.
      embedding_matrix[index+1,] <- embedding_vector
  }
}
```

## **Model Building**

- Building a training model similar to original model with one embedding layer and one dense layer.

```{r}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

summary(model)
```

- Load the pretrained GloVe embeddings in the model

```{r}
get_layer(model, index = 1) %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)
```

- Evaluating validation data on train model

```{r}
history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)
plot(history)
```

## **Hypertuning**

- Hypertunning the model based on the epoch  up to the elbow point.

```{r}
model %>% fit(
  x_train,
  y_train,
  epochs = 2,
  batch_size = 32)
```

- Testing the model on entire data set.

```{r}
Pretrain_result <- model %>%  evaluate(data,y_data)
Pretrain_result 
```

**The model have very less accuracy of 56.18% for total data and for validation data its 55.48%.**

# **Embedding Layer**

Using an embedding layer and Classifier on the IMDB data to build a model

```{r}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)
```

Evaluating validation data on train model

```{r}
history1 <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

plot(history1)
```

## **Hypertuning and building final model**

```{r}
model %>% fit(
  x_train,
  y_train,
  epochs = 2,
  batch_size = 32)
```

- Evalutaing model on validation data.

- Evaluating the entire data set on the final model to get the final result using embedding layer.

```{r}
Embedding_result <- model %>%  evaluate(data, y_data)
Embedding_result
```

The model performs poorly for the validation data as well as the test data set. The fewer sample training do not performs very well in predicting. **The Validation set accuracy is only 52.28% whereas that of final test set it is about 52.8%.**

```{r}
OriginlEmbedding <- c(0.52553, 0.75625)
OriginalPretrain <- c(0.85127,0.59076)
Result <- rbind.data.frame(Embedding_result, Pretrain_result, OriginlEmbedding, OriginalPretrain)
rownames(Result) <- c("Embedding_result", "Pretrain_result", "OriginlEmbedding", "OriginalPretrain")

Result
```

# **Summary**

As we could see in the Result Table, we could conclude that:

- **Train Set: 100 sample**
-- Since we are using very less sample set for training purpose, the model could not extract the features of the data and thus the performance is not so good.

- **Training Techniques**
-- Similarly using the pretrained layer the performance seems to be slightly better then the embedding layer. But the loss is high for Pretrained network.

- **Review cutoff length: 150 words** 
-- The length of the sample is also kept only of 150 words which also maybe one of the reason that the model have not performed well. The review may be incomplete and thus the wrong prediction.

Hence looking at the Loss and Accuracy we could conclue that the best model that was built for IMDB dataset is **Embedding Layer** model from the cut off count of 200, training sample of 10,000 where we got accuracy of **~75%** and loss is much lesser of about **52%**.

Althought, this may not be true with respect to all type of text mining and hypertuning always matters. We can always build a better model with certain hypertuning measures and by implementing methods under right circumtances.